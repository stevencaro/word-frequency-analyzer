#!/usr/bin/python3

from collections import defaultdict
from itertools import zip_longest

import re
import sys

class WordList():

    def __init__(self):
        self.word_list = {}

        with open('/home/scaro/git/word-frequency-analyzer/top-500-words') as f:
            for line in f.readlines():
                line = line.strip()
                self.word_list[line] = 0

        with open('/home/scaro/git/word-frequency-analyzer/top-100-verbs-with-conjugation') as f:
            for line in f.readlines():
                words = line.split()
                for word in words:
                    self.word_list[word] = 0

# The following is a recipe from the itertools documentation page:
# https://docs.python.org/3/library/itertools.html
def grouper(iterable, n, fillvalue=None):
    "Collect data into fixed-length chunks or blocks"
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)

if __name__ == '__main__':

    try:
        content_file = sys.argv[1]
    except IndexError:
        sys.exit()

    # The following code reads the content as one chunk of data:
    with open(content_file) as f:
        content = f.read()

    # The following code strips double quotes, single quotes, commas, periods, and colons
    # from each word in the content:
    content_words          = [ word.lower().lstrip('“"‘\'').rstrip(':,."”\'’') \
                             for word in content.split() ]

    # Calculates total word count and unique words in the content:
    count_of_total_words = len(content_words)
    content_unique_words = set(content_words)

    # Creates the frequent word word-list:
    wl = WordList()

    infrequent_words = []
    # Calculates which words in the content are in the frequent words dictionary
    for word in sorted(content_unique_words):
        if word in wl.word_list:
           wl.word_list[word] += 1
        else:
            infrequent_words.append(word)

    frequent_words   = []
    # Collects the words that were calculated to be frequent
    for word in sorted(wl.word_list):
        if wl.word_list[word] > 0:
            frequent_words.append(word)

    # Calculates and displays the totals:
    count_of_unique_words     = len(content_unique_words)
    count_of_frequent_words   = len(frequent_words)
    count_of_infrequent_words = len(infrequent_words)

    print (count_of_total_words, "total words")
    print (count_of_unique_words, "unique words")
    print (count_of_frequent_words, "frequent words")


    # Prints the frequent words in a 5 column format
    for wds in grouper(frequent_words, 5, fillvalue = ""):
        print('  {:15.15} {:15.15} {:15.15} {:15.15} {:15.15}'.format(*wds))

    print (count_of_infrequent_words, "infrequent words" )

    # Prints the infrequent words in a 5 column format
    for wds in grouper(infrequent_words, 5, fillvalue = ""):
        print('  {:15.15} {:15.15} {:15.15} {:15.15} {:15.15}'.format(*wds))

    # Prints the percentages of frequent words and infrequent words
    print('Percentage frequent words: {:.2%} '.format( count_of_frequent_words / count_of_unique_words))
    print('Percentage infrequent words: {:.2%} '.format( count_of_infrequent_words / count_of_unique_words))
